{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHIxHA6R1JSYnaoigAElLo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashutoshgithubs/Machine-Learning/blob/main/textPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6f3YRvSxpjc",
        "outputId": "eb6937ad-e58c-46db-e4ae-58dc837d8938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Original Text:\n",
            "['Natural Language Processing is fascinating! It involves computers understanding human language.', 'Machine learning algorithms are revolutionizing the way we process textual data.', 'Text preprocessing is crucial for NLP tasks like sentiment analysis and classification.', 'Tokenization, stemming, and lemmatization are fundamental preprocessing steps.', 'The quick brown foxes are running through the beautiful forests.']\n",
            "\n",
            "==================================================\n",
            "\n",
            "Combined Text:\n",
            "Natural Language Processing is fascinating! It involves computers understanding human language. Machine learning algorithms are revolutionizing the way we process textual data. Text preprocessing is crucial for NLP tasks like sentiment analysis and classification. Tokenization, stemming, and lemmatization are fundamental preprocessing steps. The quick brown foxes are running through the beautiful forests.\n",
            "\n",
            "==================================================\n",
            "\n",
            "After removing extra whitespaces:\n",
            "Natural Language Processing is fascinating! It involves computers understanding human language. Machine learning algorithms are revolutionizing the way we process textual data. Text preprocessing is crucial for NLP tasks like sentiment analysis and classification. Tokenization, stemming, and lemmatization are fundamental preprocessing steps. The quick brown foxes are running through the beautiful forests.\n",
            "\n",
            "==================================================\n",
            "\n",
            "Step 1 - Lowercase:\n",
            "natural language processing is fascinating! it involves computers understanding human language. machine learning algorithms are revolutionizing the way we process textual data. text preprocessing is crucial for nlp tasks like sentiment analysis and classification. tokenization, stemming, and lemmatization are fundamental preprocessing steps. the quick brown foxes are running through the beautiful forests.\n",
            "\n",
            "Step 2 - Remove punctuation:\n",
            "natural language processing is fascinating it involves computers understanding human language machine learning algorithms are revolutionizing the way we process textual data text preprocessing is crucial for nlp tasks like sentiment analysis and classification tokenization stemming and lemmatization are fundamental preprocessing steps the quick brown foxes are running through the beautiful forests\n",
            "\n",
            "Step 3.1- Sent Tokenization:\n",
            "['natural language processing is fascinating it involves computers understanding human language machine learning algorithms are revolutionizing the way we process textual data text preprocessing is crucial for nlp tasks like sentiment analysis and classification tokenization stemming and lemmatization are fundamental preprocessing steps the quick brown foxes are running through the beautiful forests']\n",
            "\n",
            "Step 3.2 - Word Tokenization:\n",
            "['natural', 'language', 'processing', 'is', 'fascinating', 'it', 'involves', 'computers', 'understanding', 'human', 'language', 'machine', 'learning', 'algorithms', 'are', 'revolutionizing', 'the', 'way', 'we', 'process', 'textual', 'data', 'text', 'preprocessing', 'is', 'crucial', 'for', 'nlp', 'tasks', 'like', 'sentiment', 'analysis', 'and', 'classification', 'tokenization', 'stemming', 'and', 'lemmatization', 'are', 'fundamental', 'preprocessing', 'steps', 'the', 'quick', 'brown', 'foxes', 'are', 'running', 'through', 'the', 'beautiful', 'forests']\n",
            "\n",
            "Step 4 - Remove stopwords:\n",
            "['natural', 'language', 'processing', 'fascinating', 'involves', 'computers', 'understanding', 'human', 'language', 'machine', 'learning', 'algorithms', 'revolutionizing', 'way', 'process', 'textual', 'data', 'text', 'preprocessing', 'crucial', 'nlp', 'tasks', 'like', 'sentiment', 'analysis', 'classification', 'tokenization', 'stemming', 'lemmatization', 'fundamental', 'preprocessing', 'steps', 'quick', 'brown', 'foxes', 'running', 'beautiful', 'forests']\n",
            "\n",
            "Step 5 - Stemming:\n",
            "['natur', 'languag', 'process', 'fascin', 'involv', 'comput', 'understand', 'human', 'languag', 'machin', 'learn', 'algorithm', 'revolution', 'way', 'process', 'textual', 'data', 'text', 'preprocess', 'crucial', 'nlp', 'task', 'like', 'sentiment', 'analysi', 'classif', 'token', 'stem', 'lemmat', 'fundament', 'preprocess', 'step', 'quick', 'brown', 'fox', 'run', 'beauti', 'forest']\n",
            "\n",
            "Step 6 - Lemmatization:\n",
            "['natural', 'language', 'processing', 'fascinating', 'involves', 'computer', 'understanding', 'human', 'language', 'machine', 'learning', 'algorithm', 'revolutionizing', 'way', 'process', 'textual', 'data', 'text', 'preprocessing', 'crucial', 'nlp', 'task', 'like', 'sentiment', 'analysis', 'classification', 'tokenization', 'stemming', 'lemmatization', 'fundamental', 'preprocessing', 'step', 'quick', 'brown', 'fox', 'running', 'beautiful', 'forest']\n",
            "\n",
            "==================================================\n",
            "FINAL RESULTS:\n",
            "Original: Natural Language Processing is fascinating! It involves computers understanding human language. Machine learning algorithms are revolutionizing the way we process textual data. Text preprocessing is crucial for NLP tasks like sentiment analysis and classification. Tokenization, stemming, and lemmatization are fundamental preprocessing steps. The quick brown foxes are running through the beautiful forests.\n",
            "Stemmed: ['natur', 'languag', 'process', 'fascin', 'involv', 'comput', 'understand', 'human', 'languag', 'machin', 'learn', 'algorithm', 'revolution', 'way', 'process', 'textual', 'data', 'text', 'preprocess', 'crucial', 'nlp', 'task', 'like', 'sentiment', 'analysi', 'classif', 'token', 'stem', 'lemmat', 'fundament', 'preprocess', 'step', 'quick', 'brown', 'fox', 'run', 'beauti', 'forest']\n",
            "Lemmatized: ['natural', 'language', 'processing', 'fascinating', 'involves', 'computer', 'understanding', 'human', 'language', 'machine', 'learning', 'algorithm', 'revolutionizing', 'way', 'process', 'textual', 'data', 'text', 'preprocessing', 'crucial', 'nlp', 'task', 'like', 'sentiment', 'analysis', 'classification', 'tokenization', 'stemming', 'lemmatization', 'fundamental', 'preprocessing', 'step', 'quick', 'brown', 'fox', 'running', 'beautiful', 'forest']\n",
            "==================================================\n",
            "Word Frequencies:\n",
            "natural: 1\n",
            "language: 2\n",
            "processing: 1\n",
            "fascinating: 1\n",
            "involves: 1\n",
            "computer: 1\n",
            "understanding: 1\n",
            "human: 1\n",
            "machine: 1\n",
            "learning: 1\n",
            "algorithm: 1\n",
            "revolutionizing: 1\n",
            "way: 1\n",
            "process: 1\n",
            "textual: 1\n",
            "data: 1\n",
            "text: 1\n",
            "preprocessing: 2\n",
            "crucial: 1\n",
            "nlp: 1\n",
            "task: 1\n",
            "like: 1\n",
            "sentiment: 1\n",
            "analysis: 1\n",
            "classification: 1\n",
            "tokenization: 1\n",
            "stemming: 1\n",
            "lemmatization: 1\n",
            "fundamental: 1\n",
            "step: 1\n",
            "quick: 1\n",
            "brown: 1\n",
            "fox: 1\n",
            "running: 1\n",
            "beautiful: 1\n",
            "forest: 1\n",
            "\n",
            "Vocabulary Size: 36\n",
            "Total Words: 38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install required packages\n",
        "!pip install nltk\n",
        "\n",
        "# Import required libraries\n",
        "import nltk\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Import specific functions\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Sample text for preprocessing\n",
        "# Sample texts for preprocessing\n",
        "text = [\n",
        "    \"Natural Language Processing is fascinating! It involves computers understanding human language.\",\n",
        "    \"Machine learning algorithms are revolutionizing the way we process textual data.\",\n",
        "    \"Text preprocessing is crucial for NLP tasks like sentiment analysis and classification.\",\n",
        "    \"Tokenization, stemming, and lemmatization are fundamental preprocessing steps.\",\n",
        "    \"The quick brown foxes are running through the beautiful forests.\"\n",
        "]\n",
        "\n",
        "full_text = text\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "text = \" \".join(text)\n",
        "print(\"Combined Text:\")\n",
        "print(text)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Removing extra whitespaces\n",
        "text = ' '.join(text.split())\n",
        "print(\"After removing extra whitespaces:\")\n",
        "print(text)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Step 1: Convert to lowercase\n",
        "text_lower = text.lower()\n",
        "print(\"Step 1 - Lowercase:\")\n",
        "print(text_lower)\n",
        "\n",
        "# Step 2: Remove punctuation and numbers\n",
        "text_clean = re.sub(r'[^a-zA-Z\\s]', '', text_lower)\n",
        "print(\"\\nStep 2 - Remove punctuation:\")\n",
        "print(text_clean)\n",
        "\n",
        "# Step 3: Tokenization\n",
        "# Tokenize Sentence\n",
        "sentences = sent_tokenize(text_clean)\n",
        "print(\"\\nStep 3.1- Sent Tokenization:\")\n",
        "print(sentences)\n",
        "# Tokenize Word\n",
        "tokens = word_tokenize(text_clean)\n",
        "print(\"\\nStep 3.2 - Word Tokenization:\")\n",
        "print(tokens)\n",
        "# Step 4: Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens_no_stopwords = []\n",
        "for word in tokens:\n",
        "    if word not in stop_words:\n",
        "        tokens_no_stopwords.append(word)\n",
        "\n",
        "print(\"\\nStep 4 - Remove stopwords:\")\n",
        "print(tokens_no_stopwords)\n",
        "\n",
        "# Step 5: Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = []\n",
        "for word in tokens_no_stopwords:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    stemmed_words.append(stemmed_word)\n",
        "\n",
        "print(\"\\nStep 5 - Stemming:\")\n",
        "print(stemmed_words)\n",
        "\n",
        "# Step 6: Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = []\n",
        "for word in tokens_no_stopwords:\n",
        "    lemmatized_word = lemmatizer.lemmatize(word)\n",
        "    lemmatized_words.append(lemmatized_word)\n",
        "\n",
        "print(\"\\nStep 6 - Lemmatization:\")\n",
        "print(lemmatized_words)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL RESULTS:\")\n",
        "print(\"Original:\", text)\n",
        "print(\"Stemmed:\", stemmed_words)\n",
        "print(\"Lemmatized:\", lemmatized_words)\n",
        "print(\"=\"*50)\n",
        "\n",
        "#Step 5: Pipelining\n",
        "# Step 6:\n",
        "# Create BOW from lemmatized words\n",
        "bow_counter = Counter(lemmatized_words)\n",
        "print(\"Word Frequencies:\")\n",
        "for word, count in bow_counter.items():\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(f\"\\nVocabulary Size: {len(bow_counter)}\")\n",
        "print(f\"Total Words: {sum(bow_counter.values())}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sCAtTWznCa1",
        "outputId": "b579cc51-f78e-4445-ca18-3c1bdc2b8fe8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural Language Processing is fascinating! It involves computers understanding human language.', 'Machine learning algorithms are revolutionizing the way we process textual data.', 'Text preprocessing is crucial for NLP tasks like sentiment analysis and classification.', 'Tokenization, stemming, and lemmatization are fundamental preprocessing steps.', 'The quick brown foxes are running through the beautiful forests.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "for i in range(len(full_text)):\n",
        "  text_clean = re.sub(r'[^a-zA-Z\\s]', ' ', full_text[i])\n",
        "  text_clean = ' '.join(text_clean.split())\n",
        "  text_clean = text_clean.lower()\n",
        "  corpus.append(text_clean)\n"
      ],
      "metadata": {
        "id": "AZ1UNhrisdUl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awl6PlOGtCvz",
        "outputId": "8891e93e-9fd7-4bf5-d2b5-39639b54e5b5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['natural language processing is fascinating it involves computers understanding human language',\n",
              " 'machine learning algorithms are revolutionizing the way we process textual data',\n",
              " 'text preprocessing is crucial for nlp tasks like sentiment analysis and classification',\n",
              " 'tokenization stemming and lemmatization are fundamental preprocessing steps',\n",
              " 'the quick brown foxes are running through the beautiful forests']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bag of Words"
      ],
      "metadata": {
        "id": "0URWDUfZw6gJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "om72Ld07tNlc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams"
      ],
      "metadata": {
        "id": "Zrmnw0-kwFHx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert list of words back to a single string because lemmatized_words is a list of words\n",
        "lemmatized_text = ' '.join(lemmatized_words)\n",
        "\n",
        "# Put it in a list (CountVectorizer expects a list of documents)\n",
        "cv = CountVectorizer(binary=True, ngram_range=(3,3))\n",
        "X = cv.fit_transform([lemmatized_text])"
      ],
      "metadata": {
        "id": "ENTnVv5etqoB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = cv.fit_transform(lemmatized_words)"
      ],
      "metadata": {
        "id": "0yu4iUKczqam"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.vocabulary_"
      ],
      "metadata": {
        "id": "hC7hdt73u-YZ",
        "outputId": "50619a66-420b-4b08-ebdb-c1ea362dbdc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'natural language processing': 18,\n",
              " 'language processing fascinating': 13,\n",
              " 'processing fascinating involves': 23,\n",
              " 'fascinating involves computer': 7,\n",
              " 'involves computer understanding': 11,\n",
              " 'computer understanding human': 4,\n",
              " 'understanding human language': 34,\n",
              " 'human language machine': 10,\n",
              " 'language machine learning': 12,\n",
              " 'machine learning algorithm': 17,\n",
              " 'learning algorithm revolutionizing': 14,\n",
              " 'algorithm revolutionizing way': 0,\n",
              " 'revolutionizing way process': 25,\n",
              " 'way process textual': 35,\n",
              " 'process textual data': 22,\n",
              " 'textual data text': 32,\n",
              " 'data text preprocessing': 6,\n",
              " 'text preprocessing crucial': 31,\n",
              " 'preprocessing crucial nlp': 20,\n",
              " 'crucial nlp task': 5,\n",
              " 'nlp task like': 19,\n",
              " 'task like sentiment': 30,\n",
              " 'like sentiment analysis': 16,\n",
              " 'sentiment analysis classification': 27,\n",
              " 'analysis classification tokenization': 1,\n",
              " 'classification tokenization stemming': 3,\n",
              " 'tokenization stemming lemmatization': 33,\n",
              " 'stemming lemmatization fundamental': 28,\n",
              " 'lemmatization fundamental preprocessing': 15,\n",
              " 'fundamental preprocessing step': 9,\n",
              " 'preprocessing step quick': 21,\n",
              " 'step quick brown': 29,\n",
              " 'quick brown fox': 24,\n",
              " 'brown fox running': 2,\n",
              " 'fox running beautiful': 8,\n",
              " 'running beautiful forest': 26}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0].toarray()"
      ],
      "metadata": {
        "id": "XlHAY7SJvGnA",
        "outputId": "9607bba4-2085-493e-a7e8-a169380b82cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TFIDF"
      ],
      "metadata": {
        "id": "g2ylLRicz_qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "d0zHRbg2xW4C"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = TfidfVectorizer(ngram_range=(3,3))\n",
        "X = cv.fit_transform([lemmatized_text])"
      ],
      "metadata": {
        "id": "VuciSjqK0I27"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0].toarray()"
      ],
      "metadata": {
        "id": "fI55ztlS0cbr",
        "outputId": "7c6d1027-07e6-422d-a3b1-c0733c59099a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
              "        0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
              "        0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
              "        0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
              "        0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
              "        0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
              "        0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
              "        0.16666667]])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "52y8TlGZ0vke"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}