{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPENeTGKrwEZ3WA+SzOYO7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashutoshgithubs/Machine-Learning/blob/main/textPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6f3YRvSxpjc",
        "outputId": "dc372617-4440-4fe4-f58f-71de4630a0d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Original Text:\n",
            "['Natural Language Processing is fascinating! It involves computers understanding human language.', 'Machine learning algorithms are revolutionizing the way we process textual data.', 'Text preprocessing is crucial for NLP tasks like sentiment analysis and classification.', 'Tokenization, stemming, and lemmatization are fundamental preprocessing steps.', 'The quick brown foxes are running through the beautiful forests.']\n",
            "\n",
            "==================================================\n",
            "\n",
            "Combined Text:\n",
            "Natural Language Processing is fascinating! It involves computers understanding human language. Machine learning algorithms are revolutionizing the way we process textual data. Text preprocessing is crucial for NLP tasks like sentiment analysis and classification. Tokenization, stemming, and lemmatization are fundamental preprocessing steps. The quick brown foxes are running through the beautiful forests.\n",
            "\n",
            "==================================================\n",
            "\n",
            "After removing extra whitespaces:\n",
            "Natural Language Processing is fascinating! It involves computers understanding human language. Machine learning algorithms are revolutionizing the way we process textual data. Text preprocessing is crucial for NLP tasks like sentiment analysis and classification. Tokenization, stemming, and lemmatization are fundamental preprocessing steps. The quick brown foxes are running through the beautiful forests.\n",
            "\n",
            "==================================================\n",
            "\n",
            "Step 1 - Lowercase:\n",
            "natural language processing is fascinating! it involves computers understanding human language. machine learning algorithms are revolutionizing the way we process textual data. text preprocessing is crucial for nlp tasks like sentiment analysis and classification. tokenization, stemming, and lemmatization are fundamental preprocessing steps. the quick brown foxes are running through the beautiful forests.\n",
            "\n",
            "Step 2 - Remove punctuation:\n",
            "natural language processing is fascinating it involves computers understanding human language machine learning algorithms are revolutionizing the way we process textual data text preprocessing is crucial for nlp tasks like sentiment analysis and classification tokenization stemming and lemmatization are fundamental preprocessing steps the quick brown foxes are running through the beautiful forests\n",
            "\n",
            "Step 3.1- Sent Tokenization:\n",
            "['natural language processing is fascinating it involves computers understanding human language machine learning algorithms are revolutionizing the way we process textual data text preprocessing is crucial for nlp tasks like sentiment analysis and classification tokenization stemming and lemmatization are fundamental preprocessing steps the quick brown foxes are running through the beautiful forests']\n",
            "\n",
            "Step 3.2 - Word Tokenization:\n",
            "['natural', 'language', 'processing', 'is', 'fascinating', 'it', 'involves', 'computers', 'understanding', 'human', 'language', 'machine', 'learning', 'algorithms', 'are', 'revolutionizing', 'the', 'way', 'we', 'process', 'textual', 'data', 'text', 'preprocessing', 'is', 'crucial', 'for', 'nlp', 'tasks', 'like', 'sentiment', 'analysis', 'and', 'classification', 'tokenization', 'stemming', 'and', 'lemmatization', 'are', 'fundamental', 'preprocessing', 'steps', 'the', 'quick', 'brown', 'foxes', 'are', 'running', 'through', 'the', 'beautiful', 'forests']\n",
            "\n",
            "Step 4 - Remove stopwords:\n",
            "['natural', 'language', 'processing', 'fascinating', 'involves', 'computers', 'understanding', 'human', 'language', 'machine', 'learning', 'algorithms', 'revolutionizing', 'way', 'process', 'textual', 'data', 'text', 'preprocessing', 'crucial', 'nlp', 'tasks', 'like', 'sentiment', 'analysis', 'classification', 'tokenization', 'stemming', 'lemmatization', 'fundamental', 'preprocessing', 'steps', 'quick', 'brown', 'foxes', 'running', 'beautiful', 'forests']\n",
            "\n",
            "Step 5 - Stemming:\n",
            "['natur', 'languag', 'process', 'fascin', 'involv', 'comput', 'understand', 'human', 'languag', 'machin', 'learn', 'algorithm', 'revolution', 'way', 'process', 'textual', 'data', 'text', 'preprocess', 'crucial', 'nlp', 'task', 'like', 'sentiment', 'analysi', 'classif', 'token', 'stem', 'lemmat', 'fundament', 'preprocess', 'step', 'quick', 'brown', 'fox', 'run', 'beauti', 'forest']\n",
            "\n",
            "Step 6 - Lemmatization:\n",
            "['natural', 'language', 'processing', 'fascinating', 'involves', 'computer', 'understanding', 'human', 'language', 'machine', 'learning', 'algorithm', 'revolutionizing', 'way', 'process', 'textual', 'data', 'text', 'preprocessing', 'crucial', 'nlp', 'task', 'like', 'sentiment', 'analysis', 'classification', 'tokenization', 'stemming', 'lemmatization', 'fundamental', 'preprocessing', 'step', 'quick', 'brown', 'fox', 'running', 'beautiful', 'forest']\n",
            "\n",
            "==================================================\n",
            "FINAL RESULTS:\n",
            "Original: Natural Language Processing is fascinating! It involves computers understanding human language. Machine learning algorithms are revolutionizing the way we process textual data. Text preprocessing is crucial for NLP tasks like sentiment analysis and classification. Tokenization, stemming, and lemmatization are fundamental preprocessing steps. The quick brown foxes are running through the beautiful forests.\n",
            "Stemmed: ['natur', 'languag', 'process', 'fascin', 'involv', 'comput', 'understand', 'human', 'languag', 'machin', 'learn', 'algorithm', 'revolution', 'way', 'process', 'textual', 'data', 'text', 'preprocess', 'crucial', 'nlp', 'task', 'like', 'sentiment', 'analysi', 'classif', 'token', 'stem', 'lemmat', 'fundament', 'preprocess', 'step', 'quick', 'brown', 'fox', 'run', 'beauti', 'forest']\n",
            "Lemmatized: ['natural', 'language', 'processing', 'fascinating', 'involves', 'computer', 'understanding', 'human', 'language', 'machine', 'learning', 'algorithm', 'revolutionizing', 'way', 'process', 'textual', 'data', 'text', 'preprocessing', 'crucial', 'nlp', 'task', 'like', 'sentiment', 'analysis', 'classification', 'tokenization', 'stemming', 'lemmatization', 'fundamental', 'preprocessing', 'step', 'quick', 'brown', 'fox', 'running', 'beautiful', 'forest']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install required packages\n",
        "!pip install nltk\n",
        "\n",
        "# Import required libraries\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Import specific functions\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Sample text for preprocessing\n",
        "# Sample texts for preprocessing\n",
        "text = [\n",
        "    \"Natural Language Processing is fascinating! It involves computers understanding human language.\",\n",
        "    \"Machine learning algorithms are revolutionizing the way we process textual data.\",\n",
        "    \"Text preprocessing is crucial for NLP tasks like sentiment analysis and classification.\",\n",
        "    \"Tokenization, stemming, and lemmatization are fundamental preprocessing steps.\",\n",
        "    \"The quick brown foxes are running through the beautiful forests.\"\n",
        "]\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "text = \" \".join(text)\n",
        "print(\"Combined Text:\")\n",
        "print(text)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Removing extra whitespaces\n",
        "text = ' '.join(text.split())\n",
        "print(\"After removing extra whitespaces:\")\n",
        "print(text)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Step 1: Convert to lowercase\n",
        "text_lower = text.lower()\n",
        "print(\"Step 1 - Lowercase:\")\n",
        "print(text_lower)\n",
        "\n",
        "# Step 2: Remove punctuation and numbers\n",
        "text_clean = re.sub(r'[^a-zA-Z\\s]', '', text_lower)\n",
        "print(\"\\nStep 2 - Remove punctuation:\")\n",
        "print(text_clean)\n",
        "\n",
        "# Step 3: Tokenization\n",
        "# Tokenize Sentence\n",
        "sentences = sent_tokenize(text_clean)\n",
        "print(\"\\nStep 3.1- Sent Tokenization:\")\n",
        "print(sentences)\n",
        "# Tokenize Word\n",
        "tokens = word_tokenize(text_clean)\n",
        "print(\"\\nStep 3.2 - Word Tokenization:\")\n",
        "print(tokens)\n",
        "# Step 4: Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens_no_stopwords = []\n",
        "for word in tokens:\n",
        "    if word not in stop_words:\n",
        "        tokens_no_stopwords.append(word)\n",
        "\n",
        "print(\"\\nStep 4 - Remove stopwords:\")\n",
        "print(tokens_no_stopwords)\n",
        "\n",
        "# Step 5: Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = []\n",
        "for word in tokens_no_stopwords:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    stemmed_words.append(stemmed_word)\n",
        "\n",
        "print(\"\\nStep 5 - Stemming:\")\n",
        "print(stemmed_words)\n",
        "\n",
        "# Step 6: Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = []\n",
        "for word in tokens_no_stopwords:\n",
        "    lemmatized_word = lemmatizer.lemmatize(word)\n",
        "    lemmatized_words.append(lemmatized_word)\n",
        "\n",
        "print(\"\\nStep 6 - Lemmatization:\")\n",
        "print(lemmatized_words)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL RESULTS:\")\n",
        "print(\"Original:\", text)\n",
        "print(\"Stemmed:\", stemmed_words)\n",
        "print(\"Lemmatized:\", lemmatized_words)\n",
        "\n",
        "#Step 5: Pipelining"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0mPimFA22VsZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}